{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e701531c",
   "metadata": {},
   "source": "# SOL/USD State Matrix - Exploratory Data Analysis\n\n**Asset:** SOL/USD | **Timeframe:** 15-minute candles | **Source:** Binance\n\n---\n\n### Purpose\n\nThis notebook validates and profiles the **state matrix** - a pre-computed parquet file that enriches raw OHLCV price data with three regime dimensions and triple-barrier labels. The state matrix is the single source of truth for all downstream pipeline components (backtesting, strategy generation, diagnostics).\n\n### What is the State Matrix?\n\nEach 15-minute candle is tagged with:\n\n| Column | Values | How it's computed |\n|--------|--------|-------------------|\n| `session` | ASIA, LONDON, NY, OTHER | UTC hour of the candle (ASIA 00-07, LONDON 08-12, NY 13-20, OTHER 21-23) |\n| `trend_regime` | UPTREND, DOWNTREND, CONSOLIDATION | SMA(50) slope over 3 bars, threshold of +/-0.0005 |\n| `vol_regime` | HIGH_VOL, LOW_VOL | ATR(24) vs its own SMA(20) - above = HIGH_VOL, at/below = LOW_VOL |\n| `tbm_label` | +1, -1, 0, NaN | Triple-barrier method oracle label (see Section D) |\n\nThese three regime dimensions create **4 x 3 x 2 = 24 micro-buckets**. The goal of this EDA is to understand which buckets carry a directional edge worth exploiting.\n\n### Notebook Structure\n\n- **A. Setup** - Load data, validate schema and index\n- **B. Data Quality** - Missingness, sentinel values, numeric distributions\n- **C. Regime Coverage** - How candles distribute across regime buckets (1D, 2D, 3D)\n- **D. Regime Edge** - Triple-barrier label composition per bucket (bias, actionability)\n- **E. Temporal Drift** - Whether regime distributions and edges are stable month-over-month\n- **F. Validation** - Automated consistency checks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fc7c6",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 180)\n\nINPUT_PATH = \"../data/state_matrix.parquet\"\nEXPECTED_FREQ = \"15min\"\nREQUIRED_COLUMNS = [\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"session\", \"trend_regime\", \"vol_regime\", \"tbm_label\"\n]\n\nSESSION_ORDER = [\"ASIA\", \"LONDON\", \"NY\", \"OTHER\"]\nTREND_ORDER = [\"UPTREND\", \"DOWNTREND\", \"CONSOLIDATION\"]\nVOL_ORDER = [\"HIGH_VOL\", \"LOW_VOL\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48120b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_state_matrix(df: pd.DataFrame) -> dict:\n",
    "    missing_cols = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"Index must be a pandas DatetimeIndex.\")\n",
    "\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        raise ValueError(\"Index must be sorted ascending (monotonic increasing).\")\n",
    "\n",
    "    if df.index.has_duplicates:\n",
    "        raise ValueError(\"Index contains duplicate timestamps; expected unique 15m bars.\")\n",
    "\n",
    "    allowed_labels = {-1.0, 0.0, 1.0}\n",
    "    label_values = set(pd.Series(df[\"tbm_label\"].dropna().unique()).astype(float).tolist())\n",
    "    invalid_labels = sorted(v for v in label_values if v not in allowed_labels)\n",
    "    if invalid_labels:\n",
    "        raise ValueError(f\"Invalid tbm_label values found: {invalid_labels}\")\n",
    "\n",
    "    return {\n",
    "        \"rows\": len(df),\n",
    "        \"start\": df.index.min(),\n",
    "        \"end\": df.index.max(),\n",
    "        \"n_missing_labels\": int(df[\"tbm_label\"].isna().sum()),\n",
    "    }\n",
    "\n",
    "\n",
    "def _bucket_orders_for_dims(dims: list[str]) -> dict[str, list[str]]:\n",
    "    order_map = {\n",
    "        \"session\": SESSION_ORDER,\n",
    "        \"trend_regime\": TREND_ORDER,\n",
    "        \"vol_regime\": VOL_ORDER,\n",
    "    }\n",
    "    return {d: order_map[d] for d in dims if d in order_map}\n",
    "\n",
    "\n",
    "def _expand_bucket_grid(df: pd.DataFrame, dims: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Return full cartesian grid for known regime dims so zero-count buckets are explicit.\"\"\"\n",
    "    if not dims:\n",
    "        return pd.DataFrame({\"bucket_key\": [\"GLOBAL\"], \"bucket_level\": [\"GLOBAL\"]})\n",
    "\n",
    "    orders = _bucket_orders_for_dims(dims)\n",
    "    levels = []\n",
    "    for d in dims:\n",
    "        if d in orders:\n",
    "            levels.append(orders[d])\n",
    "        else:\n",
    "            levels.append(sorted(df[d].astype(str).dropna().unique().tolist()))\n",
    "\n",
    "    grid = pd.MultiIndex.from_product(levels, names=dims).to_frame(index=False)\n",
    "    grid[\"bucket_key\"] = grid[dims].astype(str).agg(\"|\".join, axis=1)\n",
    "    grid[\"bucket_level\"] = f\"{len(dims)}D\"\n",
    "    return grid\n",
    "\n",
    "\n",
    "def bucket_key_from_dims(df: pd.DataFrame, dims: list[str]) -> pd.Series:\n",
    "    if len(dims) == 0:\n",
    "        return pd.Series([\"GLOBAL\"] * len(df), index=df.index)\n",
    "    return df[dims].astype(str).agg(\"|\".join, axis=1)\n",
    "\n",
    "\n",
    "def coverage_table(df: pd.DataFrame, dims: list[str]) -> pd.DataFrame:\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        raise ValueError(\"DataFrame is empty; cannot compute coverage.\")\n",
    "\n",
    "    if not dims:\n",
    "        return pd.DataFrame({\n",
    "            \"count\": [total],\n",
    "            \"coverage_pct\": [100.0],\n",
    "            \"bucket_key\": [\"GLOBAL\"],\n",
    "            \"bucket_level\": [\"GLOBAL\"],\n",
    "        })\n",
    "\n",
    "    grouped = df.groupby(dims, dropna=False).size().rename(\"count\").reset_index()\n",
    "    grouped[\"coverage_pct\"] = grouped[\"count\"] / total * 100.0\n",
    "    grouped[\"bucket_key\"] = grouped[dims].astype(str).agg(\"|\".join, axis=1)\n",
    "    grouped[\"bucket_level\"] = f\"{len(dims)}D\"\n",
    "\n",
    "    # Ensure all expected regime combinations are present with zero count.\n",
    "    grid = _expand_bucket_grid(df, dims)\n",
    "    out = grid.merge(grouped, on=dims + [\"bucket_key\", \"bucket_level\"], how=\"left\")\n",
    "    out[\"count\"] = out[\"count\"].fillna(0).astype(int)\n",
    "    out[\"coverage_pct\"] = out[\"coverage_pct\"].fillna(0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def label_profile(df: pd.DataFrame, dims: list[str]) -> pd.DataFrame:\n",
    "    base = coverage_table(df, dims)\n",
    "\n",
    "    if not dims:\n",
    "        p_plus = float((df[\"tbm_label\"] == 1).mean())\n",
    "        p_minus = float((df[\"tbm_label\"] == -1).mean())\n",
    "        p_zero = float((df[\"tbm_label\"] == 0).mean())\n",
    "        p_nan = float(df[\"tbm_label\"].isna().mean())\n",
    "        out = base.copy()\n",
    "        out[\"p_plus\"] = p_plus\n",
    "        out[\"p_minus\"] = p_minus\n",
    "        out[\"p_zero\"] = p_zero\n",
    "        out[\"p_nan\"] = p_nan\n",
    "    else:\n",
    "        grouped = df.groupby(dims, dropna=False)[\"tbm_label\"]\n",
    "        p_plus = grouped.apply(lambda s: (s == 1).mean()).reset_index(name=\"p_plus\")\n",
    "        p_minus = grouped.apply(lambda s: (s == -1).mean()).reset_index(name=\"p_minus\")\n",
    "        p_zero = grouped.apply(lambda s: (s == 0).mean()).reset_index(name=\"p_zero\")\n",
    "        p_nan = grouped.apply(lambda s: s.isna().mean()).reset_index(name=\"p_nan\")\n",
    "\n",
    "        out = base.merge(p_plus, on=dims, how=\"left\")\n",
    "        out = out.merge(p_minus, on=dims, how=\"left\")\n",
    "        out = out.merge(p_zero, on=dims, how=\"left\")\n",
    "        out = out.merge(p_nan, on=dims, how=\"left\")\n",
    "        out[[\"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\"]] = out[[\"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\"]].fillna(0.0)\n",
    "\n",
    "    out[\"bias\"] = out[\"p_plus\"] - out[\"p_minus\"]\n",
    "    out[\"actionability\"] = 1.0 - out[\"p_zero\"] - out[\"p_nan\"]\n",
    "    out[\"prob_sum\"] = out[[\"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\"]].sum(axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def monthly_regime_drift(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    tmp = df.copy()\n",
    "    tmp[\"month\"] = tmp.index.to_period(\"M\").astype(str)\n",
    "    g = tmp.groupby([\"month\", col], dropna=False).size().rename(\"count\").reset_index()\n",
    "    g[\"pct\"] = g[\"count\"] / g.groupby(\"month\")[\"count\"].transform(\"sum\") * 100.0\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_findings_table(profile_1d: pd.DataFrame, profile_2d: pd.DataFrame, profile_3d: pd.DataFrame, low_count_threshold: int) -> pd.DataFrame:\n",
    "    combined = pd.concat([profile_1d, profile_2d, profile_3d], ignore_index=True)\n",
    "    combined[\"low_count_flag\"] = combined[\"count\"] < low_count_threshold\n",
    "\n",
    "    comments = []\n",
    "    for _, r in combined.iterrows():\n",
    "        if r[\"low_count_flag\"]:\n",
    "            comments.append(\"Uncertain: low sample bucket; interpret cautiously.\")\n",
    "        elif abs(r[\"bias\"]) >= 0.08 and r[\"actionability\"] >= 0.50 and r[\"coverage_pct\"] >= 3:\n",
    "            comments.append(\"Good candidate regime: directional edge with usable actionability.\")\n",
    "        elif r[\"actionability\"] < 0.25:\n",
    "            comments.append(\"Avoid/uncertain: mostly timeout/whipsaw behavior.\")\n",
    "        elif abs(r[\"bias\"]) < 0.02:\n",
    "            comments.append(\"Neutral edge: weak directional skew.\")\n",
    "        else:\n",
    "            comments.append(\"Potentially useful; validate with strategy-specific tests.\")\n",
    "\n",
    "    combined[\"comment\"] = comments\n",
    "    cols = [\n",
    "        \"bucket_level\", \"bucket_key\", \"count\", \"coverage_pct\", \"bias\", \"actionability\",\n",
    "        \"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\", \"low_count_flag\", \"comment\"\n",
    "    ]\n",
    "    out = combined[cols].copy()\n",
    "\n",
    "    level_order = {\"1D\": 1, \"2D\": 2, \"3D\": 3, \"GLOBAL\": 0}\n",
    "    out[\"_level_sort\"] = out[\"bucket_level\"].map(level_order).fillna(99)\n",
    "    out = out.sort_values([\"_level_sort\", \"coverage_pct\", \"bias\"], ascending=[True, False, False]).drop(columns=[\"_level_sort\"]) \\\n",
    "             .reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcae4d0",
   "metadata": {},
   "source": "---\n\n## A. Setup and Data Contract\n\nLoad the state matrix and run structural validation:\n- All required columns present (`open/high/low/close/volume` + regime tags + `tbm_label`)\n- Index is a UTC DatetimeIndex, sorted ascending, with no duplicates\n- `tbm_label` values are restricted to {-1.0, 0.0, 1.0, NaN}\n- Timestamp grid is a continuous 15-minute sequence with no gaps or extras"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.read_parquet(INPUT_PATH)\n",
    "\n",
    "if not isinstance(sol.index, pd.DatetimeIndex):\n",
    "    sol.index = pd.to_datetime(sol.index, utc=True, errors=\"coerce\")\n",
    "\n",
    "sol = sol.sort_index()\n",
    "validation = validate_state_matrix(sol)\n",
    "validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_index = pd.date_range(sol.index.min(), sol.index.max(), freq=EXPECTED_FREQ)\n",
    "missing_timestamps = expected_index.difference(sol.index)\n",
    "extra_timestamps = sol.index.difference(expected_index)\n",
    "\n",
    "print(f\"Rows: {len(sol):,}\")\n",
    "print(f\"Start: {sol.index.min()}\")\n",
    "print(f\"End:   {sol.index.max()}\")\n",
    "print(f\"Missing timestamps vs expected 15m grid: {len(missing_timestamps):,}\")\n",
    "print(f\"Extra timestamps outside expected grid: {len(extra_timestamps):,}\")\n",
    "\n",
    "years_present = sorted(sol.index.year.unique().tolist())\n",
    "print(f\"Years present: {years_present}\")\n",
    "if years_present == [2025]:\n",
    "    print(\"Note: dataset currently spans only 2025. Findings may not generalize across 2023-2025.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1e521",
   "metadata": {},
   "source": "---\n\n## B. Data Quality Snapshot\n\nBefore analyzing regimes, we need to understand the health of the underlying data:\n\n- **Missingness** - Which columns have nulls, and at what rate? High null rates in `tbm_label` may indicate whipsaw candles where both long and short barriers were hit simultaneously.\n- **TBM sentinel values** - The triple-barrier labeling uses `-1` as a sentinel for exit index/duration when a candle could not be evaluated (e.g., near the end of the dataset where the forward window runs out). These are expected but must be quantified.\n- **Numeric distributions** - Descriptive stats for OHLCV and TBM outputs, including IQR-based outlier bounds to flag anomalies in price, volume, PnL, or trade duration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_tbl = pd.DataFrame({\n",
    "    \"null_count\": sol.isna().sum(),\n",
    "    \"null_pct\": sol.isna().mean() * 100,\n",
    "}).sort_values(\"null_pct\", ascending=False)\n",
    "\n",
    "tbm_edge_checks = {\n",
    "    \"tbm_long_exit_idx == -1\": int((sol.get(\"tbm_long_exit_idx\", pd.Series([], dtype=float)) == -1).sum()) if \"tbm_long_exit_idx\" in sol.columns else None,\n",
    "    \"tbm_short_exit_idx == -1\": int((sol.get(\"tbm_short_exit_idx\", pd.Series([], dtype=float)) == -1).sum()) if \"tbm_short_exit_idx\" in sol.columns else None,\n",
    "    \"tbm_long_duration == -1\": int((sol.get(\"tbm_long_duration\", pd.Series([], dtype=float)) == -1).sum()) if \"tbm_long_duration\" in sol.columns else None,\n",
    "    \"tbm_short_duration == -1\": int((sol.get(\"tbm_short_duration\", pd.Series([], dtype=float)) == -1).sum()) if \"tbm_short_duration\" in sol.columns else None,\n",
    "    \"tbm_label is NaN\": int(sol[\"tbm_label\"].isna().sum()),\n",
    "}\n",
    "\n",
    "missing_tbl.head(20), tbm_edge_checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pd\" not in globals() or \"sol\" not in globals():\n",
    "    raise RuntimeError(\"Run setup/data-load cells first (imports + `sol`).\")\n",
    "\n",
    "numeric_cols = [c for c in [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"tbm_label\", \"tbm_long_pnl\", \"tbm_short_pnl\",\n",
    "    \"tbm_long_duration\", \"tbm_short_duration\"\n",
    "] if c in sol.columns]\n",
    "\n",
    "if not numeric_cols:\n",
    "    raise ValueError(\"No numeric columns found for summary.\")\n",
    "\n",
    "summary = sol[numeric_cols].describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).T\n",
    "\n",
    "# Robust quantile references in case pandas display labels vary by version\n",
    "q25_col = \"25%\" if \"25%\" in summary.columns else None\n",
    "q75_col = \"75%\" if \"75%\" in summary.columns else None\n",
    "if q25_col is None or q75_col is None:\n",
    "    q25 = sol[numeric_cols].quantile(0.25)\n",
    "    q75 = sol[numeric_cols].quantile(0.75)\n",
    "else:\n",
    "    q25 = summary[q25_col]\n",
    "    q75 = summary[q75_col]\n",
    "\n",
    "summary[\"iqr\"] = q75 - q25\n",
    "summary[\"outlier_low\"] = q25 - 1.5 * summary[\"iqr\"]\n",
    "summary[\"outlier_high\"] = q75 + 1.5 * summary[\"iqr\"]\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a015a",
   "metadata": {},
   "source": "---\n\n## C. Regime Coverage\n\nCoverage measures what percentage of total candles fall into each regime bucket. This matters because:\n\n- **Imbalanced coverage** means some regime combinations are rare and statistically unreliable\n- **Zero-count buckets** would indicate regime combinations that never occur in practice\n- We analyze coverage at three levels of granularity:\n  - **1D** - Each dimension independently (session, trend, volatility)\n  - **2D** - Pairwise combinations (session x trend, session x vol, trend x vol)\n  - **3D** - Full cross: all 24 micro-buckets (session x trend x vol)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6210edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "if \"coverage_table\" not in globals() or \"sol\" not in globals():\n",
    "    raise RuntimeError(\"Run setup/helper cells first (coverage_table + sol).\")\n",
    "\n",
    "cov_1d = {\n",
    "    \"session\": coverage_table(sol, [\"session\"]),\n",
    "    \"trend_regime\": coverage_table(sol, [\"trend_regime\"]),\n",
    "    \"vol_regime\": coverage_table(sol, [\"vol_regime\"]),\n",
    "}\n",
    "\n",
    "for key, tbl in cov_1d.items():\n",
    "    plot_tbl = tbl.sort_values(\"coverage_pct\", ascending=False).copy()\n",
    "    fig = px.bar(\n",
    "        plot_tbl,\n",
    "        x=key,\n",
    "        y=\"coverage_pct\",\n",
    "        text=plot_tbl[\"coverage_pct\"].round(2),\n",
    "        title=f\"1D Coverage — {key}\",\n",
    "    )\n",
    "    fig.update_traces(textposition=\"outside\")\n",
    "    fig.update_layout(yaxis_title=\"Coverage (%)\", xaxis_title=key, showlegend=False)\n",
    "    fig.show()\n",
    "\n",
    "cov_1d[\"session\"], cov_1d[\"trend_regime\"], cov_1d[\"vol_regime\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f80813",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "if \"coverage_table\" not in globals() or \"sol\" not in globals():\n",
    "    raise RuntimeError(\"Run setup/helper cells first (coverage_table + sol).\")\n",
    "\n",
    "pairs = [\n",
    "    (\"session\", \"trend_regime\"),\n",
    "    (\"session\", \"vol_regime\"),\n",
    "    (\"trend_regime\", \"vol_regime\"),\n",
    "]\n",
    "\n",
    "order_map = {\n",
    "    \"session\": SESSION_ORDER,\n",
    "    \"trend_regime\": TREND_ORDER,\n",
    "    \"vol_regime\": VOL_ORDER,\n",
    "}\n",
    "\n",
    "for a, b in pairs:\n",
    "    tbl = coverage_table(sol, [a, b])\n",
    "    pivot = tbl.pivot(index=a, columns=b, values=\"coverage_pct\").reindex(\n",
    "        index=order_map.get(a, sorted(tbl[a].dropna().unique())),\n",
    "        columns=order_map.get(b, sorted(tbl[b].dropna().unique())),\n",
    "    ).fillna(0)\n",
    "\n",
    "    fig = px.imshow(\n",
    "        pivot,\n",
    "        text_auto=\".2f\",\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        aspect=\"auto\",\n",
    "        title=f\"2D Coverage Heatmap — {a} x {b}\",\n",
    "    )\n",
    "    fig.update_layout(coloraxis_colorbar_title=\"Coverage %\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "if \"coverage_table\" not in globals() or \"sol\" not in globals():\n",
    "    raise RuntimeError(\"Run setup/helper cells first (coverage_table + sol).\")\n",
    "\n",
    "cov_3d = coverage_table(sol, [\"session\", \"trend_regime\", \"vol_regime\"])\n",
    "cov_3d[\"session_trend\"] = cov_3d[\"session\"] + \"|\" + cov_3d[\"trend_regime\"]\n",
    "\n",
    "row_order = [f\"{s}|{t}\" for s in SESSION_ORDER for t in TREND_ORDER]\n",
    "heat_3d = cov_3d.pivot(index=\"session_trend\", columns=\"vol_regime\", values=\"coverage_pct\").reindex(\n",
    "    index=row_order,\n",
    "    columns=VOL_ORDER,\n",
    ").fillna(0)\n",
    "\n",
    "fig = px.imshow(\n",
    "    heat_3d,\n",
    "    text_auto=\".2f\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    aspect=\"auto\",\n",
    "    title=\"3D Coverage Matrix — (session|trend) x vol_regime\",\n",
    ")\n",
    "fig.update_layout(coloraxis_colorbar_title=\"Coverage %\")\n",
    "fig.show()\n",
    "\n",
    "cov_3d.sort_values(\"coverage_pct\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89227072",
   "metadata": {},
   "source": "---\n\n## D. Regime Edge (TBM Labels)\n\nThe **triple-barrier method (TBM)** evaluates every candle by simultaneously simulating a long and short trade with:\n- **Take profit** at entry +/- 2.0x ATR\n- **Stop loss** at entry -/+ 1.0x ATR\n- **Timeout** after 50 bars (~12.5 hours) if neither barrier is hit\n\nThe oracle label assigned to each candle:\n| Label | Meaning |\n|-------|---------|\n| **+1** | Long trade would have hit TP before SL |\n| **-1** | Short trade would have hit TP before SL |\n| **0** | Both sides timed out - no clear directional move |\n| **NaN** | Whipsaw - both long and short hit their stop losses |\n\nFor each regime bucket, we compute:\n- **`bias`** = P(+1) - P(-1) - positive means the bucket favors longs, negative favors shorts\n- **`actionability`** = 1 - P(0) - P(NaN) - how often the bucket produces a clear directional outcome vs timeouts/whipsaws\n\nA good candidate regime has **high |bias|**, **high actionability**, and **sufficient coverage** (sample size)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "required = [\"label_profile\", \"pairs\", \"sol\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Run prerequisite cells first. Missing: {missing}\")\n",
    "\n",
    "profile_1d = {\n",
    "    \"session\": label_profile(sol, [\"session\"]),\n",
    "    \"trend_regime\": label_profile(sol, [\"trend_regime\"]),\n",
    "    \"vol_regime\": label_profile(sol, [\"vol_regime\"]),\n",
    "}\n",
    "profile_2d = {f\"{a}__{b}\": label_profile(sol, [a, b]) for a, b in pairs}\n",
    "profile_3d = label_profile(sol, [\"session\", \"trend_regime\", \"vol_regime\"])\n",
    "\n",
    "for key, tbl in profile_1d.items():\n",
    "    melt = tbl[[key, \"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\"]].melt(\n",
    "        id_vars=[key], var_name=\"label_component\", value_name=\"probability\"\n",
    "    )\n",
    "    fig = px.bar(\n",
    "        melt,\n",
    "        x=key,\n",
    "        y=\"probability\",\n",
    "        color=\"label_component\",\n",
    "        barmode=\"stack\",\n",
    "        title=f\"TBM Label Composition — {key}\",\n",
    "    )\n",
    "    fig.update_layout(yaxis_title=\"Probability\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bf038",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "required = [\"profile_2d\", \"profile_3d\", \"pairs\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Run prerequisite cells first. Missing: {missing}\")\n",
    "if \"order_map\" not in globals():\n",
    "    order_map = {\n",
    "        \"session\": SESSION_ORDER,\n",
    "        \"trend_regime\": TREND_ORDER,\n",
    "        \"vol_regime\": VOL_ORDER,\n",
    "    }\n",
    "\n",
    "for a, b in pairs:\n",
    "    key = f\"{a}__{b}\"\n",
    "    tbl = profile_2d[key]\n",
    "    pivot = tbl.pivot(index=a, columns=b, values=\"bias\").reindex(\n",
    "        index=order_map.get(a, sorted(tbl[a].dropna().unique())),\n",
    "        columns=order_map.get(b, sorted(tbl[b].dropna().unique())),\n",
    "    )\n",
    "    fig = px.imshow(\n",
    "        pivot,\n",
    "        text_auto=\".3f\",\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        aspect=\"auto\",\n",
    "        title=f\"Directional Bias (p_plus - p_minus) — {a} x {b}\",\n",
    "    )\n",
    "    fig.update_layout(coloraxis_colorbar_title=\"Bias\")\n",
    "    fig.show()\n",
    "\n",
    "profile_3d_sorted_bias = profile_3d.assign(abs_bias=profile_3d[\"bias\"].abs()).sort_values(\"abs_bias\", ascending=False)\n",
    "profile_3d_sorted_bias[[\"bucket_key\", \"count\", \"coverage_pct\", \"bias\", \"actionability\", \"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\"]].head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "if \"profile_3d\" not in globals():\n",
    "    raise RuntimeError(\"Run label-profile cells first (profile_3d is missing).\")\n",
    "\n",
    "LOW_COUNT_THRESHOLD = max(30, int(len(sol) * 0.002))\n",
    "\n",
    "act_tbl = profile_3d.copy()\n",
    "act_tbl[\"low_count_flag\"] = act_tbl[\"count\"] < LOW_COUNT_THRESHOLD\n",
    "\n",
    "fig = px.scatter(\n",
    "    act_tbl,\n",
    "    x=\"coverage_pct\",\n",
    "    y=\"actionability\",\n",
    "    color=\"bias\",\n",
    "    hover_data=[\"bucket_key\", \"count\", \"p_plus\", \"p_minus\", \"p_zero\", \"p_nan\", \"low_count_flag\"],\n",
    "    title=\"Actionability vs Coverage (3D Buckets)\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    ")\n",
    "fig.update_layout(xaxis_title=\"Coverage (%)\", yaxis_title=\"Actionability\")\n",
    "fig.show()\n",
    "\n",
    "act_tbl.sort_values([\"actionability\", \"coverage_pct\"], ascending=[False, False]).head(24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78151a2a",
   "metadata": {},
   "source": "---\n\n## E. Temporal Drift\n\nA regime edge is only useful if it persists over time. This section checks:\n\n1. **Regime distribution drift** - Do the proportions of UPTREND/DOWNTREND/CONSOLIDATION (and other dimensions) stay stable month-to-month, or do they shift dramatically? Large shifts mean the market's character is changing.\n2. **Bias drift for top buckets** - For the highest-coverage and highest-bias 3D buckets, track monthly directional bias. A bucket that flips sign frequently is unreliable regardless of its aggregate statistics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "if \"monthly_regime_drift\" not in globals() or \"sol\" not in globals():\n",
    "    raise RuntimeError(\"Run setup/helper cells first (monthly_regime_drift + sol).\")\n",
    "\n",
    "for col in [\"session\", \"trend_regime\", \"vol_regime\"]:\n",
    "    drift = monthly_regime_drift(sol, col)\n",
    "    fig = px.bar(\n",
    "        drift,\n",
    "        x=\"month\",\n",
    "        y=\"pct\",\n",
    "        color=col,\n",
    "        barmode=\"stack\",\n",
    "        title=f\"Monthly Regime Drift — {col}\",\n",
    "    )\n",
    "    fig.update_layout(yaxis_title=\"Share of candles (%)\", xaxis_title=\"Month\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px\" not in globals():\n",
    "    import plotly.express as px\n",
    "required = [\"profile_3d\", \"bucket_key_from_dims\", \"sol\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Run prerequisite cells first. Missing: {missing}\")\n",
    "\n",
    "top_by_coverage = profile_3d.nlargest(3, \"coverage_pct\")[\"bucket_key\"].tolist()\n",
    "top_by_abs_bias = profile_3d.assign(abs_bias=profile_3d[\"bias\"].abs()).nlargest(3, \"abs_bias\")[\"bucket_key\"].tolist()\n",
    "selected_keys = list(dict.fromkeys(top_by_coverage + top_by_abs_bias))\n",
    "\n",
    "tmp = sol.copy()\n",
    "tmp[\"month\"] = tmp.index.to_period(\"M\").astype(str)\n",
    "tmp[\"bucket_key\"] = bucket_key_from_dims(tmp, [\"session\", \"trend_regime\", \"vol_regime\"])\n",
    "\n",
    "monthly_bias = (\n",
    "    tmp[tmp[\"bucket_key\"].isin(selected_keys)]\n",
    "    .groupby([\"month\", \"bucket_key\"])[\"tbm_label\"]\n",
    "    .apply(lambda s: (s == 1).mean() - (s == -1).mean())\n",
    "    .rename(\"bias\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.line(\n",
    "    monthly_bias,\n",
    "    x=\"month\",\n",
    "    y=\"bias\",\n",
    "    color=\"bucket_key\",\n",
    "    markers=True,\n",
    "    title=\"Monthly Directional Bias Drift — Selected 3D Buckets\",\n",
    ")\n",
    "fig.update_layout(yaxis_title=\"Bias (p_plus - p_minus)\", xaxis_title=\"Month\")\n",
    "fig.show()\n",
    "\n",
    "monthly_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be852e2",
   "metadata": {},
   "source": "---\n\n## F. Validation and Consolidated Findings\n\nAutomated consistency checks to catch pipeline bugs or data corruption:\n\n1. Schema, index, and label integrity (via `validate_state_matrix`)\n2. Coverage counts sum to total rows at every grouping level\n3. Label probabilities (p_plus + p_minus + p_zero + p_nan) sum to 1.0 in every bucket\n4. Monthly drift computation returns non-empty results\n5. All 24 expected 3D buckets are populated (no zero-count gaps)\n\nThe **findings table** consolidates all 1D, 2D, and 3D bucket profiles with automated commentary flagging the best candidates, weak edges, and low-sample warnings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381570e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Schema test + 2) Index test + 3) Label integrity test\n",
    "_ = validate_state_matrix(sol)\n",
    "\n",
    "# 4) Coverage consistency test for grouped buckets\n",
    "for dims in [[\"session\"], [\"session\", \"trend_regime\"], [\"session\", \"trend_regime\", \"vol_regime\"]]:\n",
    "    ct = coverage_table(sol, dims)\n",
    "    assert int(ct[\"count\"].sum()) == len(sol), f\"Coverage count mismatch for {dims}\"\n",
    "\n",
    "# 5) Probability consistency test\n",
    "for prof, name in [\n",
    "    (profile_1d[\"session\"], \"1D session\"),\n",
    "    (profile_1d[\"trend_regime\"], \"1D trend\"),\n",
    "    (profile_1d[\"vol_regime\"], \"1D vol\"),\n",
    "    (profile_3d, \"3D\"),\n",
    "]:\n",
    "    assert np.allclose(prof[\"prob_sum\"].values, 1.0, atol=1e-8), f\"Probability sum mismatch in {name}\"\n",
    "\n",
    "# 6) Drift section test\n",
    "drift_test = monthly_regime_drift(sol, \"session\")\n",
    "assert len(drift_test) > 0, \"Monthly drift computation returned empty output\"\n",
    "\n",
    "# 7) Empty bucket safety check\n",
    "all_3d = pd.MultiIndex.from_product([SESSION_ORDER, TREND_ORDER, VOL_ORDER], names=[\"session\", \"trend_regime\", \"vol_regime\"])\n",
    "observed = profile_3d.set_index([\"session\", \"trend_regime\", \"vol_regime\"]) [\"count\"]\n",
    "reindexed = observed.reindex(all_3d, fill_value=0)\n",
    "print(\"Validation checks passed.\")\n",
    "print(f\"3D buckets with zero observed candles: {(reindexed == 0).sum()}\")\n",
    "print(f\"Low count threshold used for warnings: {LOW_COUNT_THRESHOLD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = build_findings_table(\n",
    "    profile_1d=pd.concat(profile_1d.values(), ignore_index=True),\n",
    "    profile_2d=pd.concat(profile_2d.values(), ignore_index=True),\n",
    "    profile_3d=profile_3d,\n",
    "    low_count_threshold=LOW_COUNT_THRESHOLD,\n",
    ")\n",
    "\n",
    "findings.head(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe3fd5",
   "metadata": {},
   "source": "---\n\n### How to Read the Findings Table\n\nEach row is a regime bucket scored on three axes:\n\n| Metric | What it tells you |\n|--------|-------------------|\n| **coverage_pct** | Sample size as % of total candles - higher is more reliable |\n| **bias** | Directional skew - positive favors longs, negative favors shorts |\n| **actionability** | How often the bucket produces a tradeable outcome (not timeout/whipsaw) |\n\n**Automated commentary rules:**\n- \"Good candidate regime\" - |bias| >= 0.08, actionability >= 0.50, coverage >= 3%\n- \"Neutral edge\" - |bias| < 0.02 regardless of other metrics\n- \"Avoid/uncertain\" - actionability < 0.25 (mostly timeouts)\n- \"Uncertain: low sample\" - below the low-count threshold (0.2% of total rows)\n\nBuckets that look good here should be validated in the temporal drift section (Section E) to confirm the edge is stable, then tested with actual strategy-specific backtests."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}